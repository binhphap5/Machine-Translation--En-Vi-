{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers, trainers, models\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ncduy/mt-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 2884451\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 11316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 11225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.remove_columns([\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word - based\n",
    "# check if there is a tokenizer file\n",
    "import os\n",
    "if not os.path.exists(\"tokenizer_en.json\") or not os.path.exists(\"tokenizer_vi.json\"):\n",
    "    tokenizer_en = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer_vi = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer_en.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    tokenizer_vi.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=100_000,\n",
    "        min_frequency=2,\n",
    "        special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
    "    )\n",
    "    # train tokenizer   \n",
    "    tokenizer_en.train_from_iterator(ds[\"train\"][\"en\"], trainer)\n",
    "    tokenizer_vi.train_from_iterator(ds[\"train\"][\"vi\"], trainer)\n",
    "    # tokenizer\n",
    "    tokenizer_en.save(\"tokenizer_en.json\")\n",
    "    tokenizer_vi.save(\"tokenizer_vi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_en = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_en.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "tokenizer_vi = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_vi.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = examples[\"en\"]\n",
    "    tgt_texts = [\"<bos>\" + sent + \"<eos>\" for sent in examples[\"vi\"]]\n",
    "    src_encodings = tokenizer_en(\n",
    "        src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN\n",
    "    )\n",
    "    tgt_encodings = tokenizer_vi(\n",
    "        tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "preprocessed_ds = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "def is_valid_sample(sample):\n",
    "    return any(token != 0 for token in sample[\"input_ids\"])\n",
    "\n",
    "preprocessed_ds = preprocessed_ds.filter(is_valid_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Smallpox also ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlán alone, including the emperor, and Peru in the 1530s, aiding the European conquerors.',\n",
       " 'vi': 'Bệnh đậu mùa cũng tàn phá México vào những năm 1520, chỉ riêng người Tenochtitlán đã có hơn 150.000 người chết, gồm cả quốc vương, và Peru vào những năm 1530, nhờ đó hỗ trợ cho những người châu Âu đi chinh phục.',\n",
       " 'source': 'WikiMatrix v1',\n",
       " 'input_ids': [85236,\n",
       "  6881,\n",
       "  35237,\n",
       "  9659,\n",
       "  6610,\n",
       "  6613,\n",
       "  7244,\n",
       "  21390,\n",
       "  15,\n",
       "  10125,\n",
       "  10993,\n",
       "  15,\n",
       "  7181,\n",
       "  6610,\n",
       "  56224,\n",
       "  54428,\n",
       "  8523,\n",
       "  15,\n",
       "  7452,\n",
       "  6613,\n",
       "  11694,\n",
       "  15,\n",
       "  6629,\n",
       "  11891,\n",
       "  6610,\n",
       "  6613,\n",
       "  7244,\n",
       "  24843,\n",
       "  15,\n",
       "  33059,\n",
       "  6613,\n",
       "  8456,\n",
       "  58201,\n",
       "  17,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [2,\n",
       "  9263,\n",
       "  9329,\n",
       "  7286,\n",
       "  6600,\n",
       "  8402,\n",
       "  7268,\n",
       "  10525,\n",
       "  6521,\n",
       "  6514,\n",
       "  6495,\n",
       "  28262,\n",
       "  15,\n",
       "  6617,\n",
       "  7393,\n",
       "  6485,\n",
       "  27157,\n",
       "  54477,\n",
       "  6475,\n",
       "  6463,\n",
       "  6622,\n",
       "  9556,\n",
       "  17,\n",
       "  7144,\n",
       "  6485,\n",
       "  6963,\n",
       "  15,\n",
       "  6936,\n",
       "  6557,\n",
       "  6789,\n",
       "  7800,\n",
       "  15,\n",
       "  6448,\n",
       "  9934,\n",
       "  6521,\n",
       "  6514,\n",
       "  6495,\n",
       "  25181,\n",
       "  15,\n",
       "  8052,\n",
       "  6496,\n",
       "  7523,\n",
       "  7278,\n",
       "  6486,\n",
       "  6514,\n",
       "  6485,\n",
       "  7270,\n",
       "  7431,\n",
       "  6594,\n",
       "  8924]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_ds['train'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNNConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size_src=15000,\n",
    "        vocab_size_tgt=15000,\n",
    "        embedding_dim=256,\n",
    "        hidden_size=256,\n",
    "        drop_out=0.15,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size_src = vocab_size_src\n",
    "        self.vocab_size_tgt = vocab_size_tgt\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_out = drop_out\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, drop_out=0.15):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            input_size, embedding_dim\n",
    "        )  # input_size = vn_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden  # B x S x H, B x H\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_size, embedding_dim\n",
    "        )  # output_size = en_vocab_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)  # with hidden is h0\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Seq2SeqRNNModel(PreTrainedModel):\n",
    "    def __init__(self, config, tokenizer_en):\n",
    "        super().__init__(config)\n",
    "        self.encoder = EncoderRNN(\n",
    "            config.vocab_size_src,\n",
    "            config.embedding_dim,\n",
    "            config.hidden_size,\n",
    "            config.drop_out,\n",
    "        )\n",
    "        self.decoder = DecoderRNN(\n",
    "            config.hidden_size, config.embedding_dim, config.vocab_size_tgt\n",
    "        )\n",
    "        self.BOS_IDX = tokenizer_en.bos_token_id\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.pad_token_id)\n",
    "\n",
    "    def forward(self, input_ids, labels):\n",
    "        batch_size, seq_len = labels.shape  # get batch_size and seq_len\n",
    "        decoder_input = torch.full((batch_size, 1), self.BOS_IDX, dtype=torch.long).to(input_ids.device)  # generate \"<bos>\" token for a batch\n",
    "        encoder_output, decoder_hidden = self.encoder(input_ids) # _, h0\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(seq_len - 1):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            decoder_input = labels[:, i + 1].unsqueeze(1)  # shift left (teacher forcing)\n",
    "\n",
    "        logits = torch.cat(decoder_outputs, dim=1)  # B x seq_len x Vocab\n",
    "        loss = self.loss_fn(logits.permute(0, 2, 1), labels[:, 1:])\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "config = Seq2SeqRNNConfig(\n",
    "    vocab_size_src=len(tokenizer_en), vocab_size_tgt=len(tokenizer_vi)\n",
    ")\n",
    "model = Seq2SeqRNNModel(config, tokenizer_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformerConfig(PretrainedConfig):\n",
    "    model_type = \"seq2seq_transformer\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size_src=100_000,\n",
    "        vocab_size_tgt=100_000,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=12,\n",
    "        max_seq_len=50,\n",
    "        drop_out=0.1,\n",
    "        **kwargs,  # This allows extra parameters for compatibility\n",
    "    ): \n",
    "        super().__init__(**kwargs)  # Ensures Hugging Face can save/load config\n",
    "        self.vocab_size_src = vocab_size_src\n",
    "        self.vocab_size_tgt = vocab_size_tgt\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.drop_out = drop_out\n",
    "\n",
    "\n",
    "class Seq2SeqTransformerModel(PreTrainedModel):\n",
    "    config_class = Seq2SeqTransformerConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embedding_src = nn.Embedding(config.vocab_size_src, config.d_model)\n",
    "        self.embedding_tgt = nn.Embedding(config.vocab_size_tgt, config.d_model)\n",
    "\n",
    "        self.position_embedding_src = nn.Embedding(config.max_seq_len, config.d_model)\n",
    "        self.position_embedding_tgt = nn.Embedding(config.max_seq_len, config.d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.num_heads,\n",
    "            num_encoder_layers=config.num_layers,\n",
    "            num_decoder_layers=config.num_layers,\n",
    "            dropout=config.drop_out,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.generator = nn.Linear(config.d_model, config.vocab_size_tgt)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def forward(self, input_ids, labels):\n",
    "        tgt_input = labels[:, :-1]  # decoder input B x seq_len\n",
    "        tgt_output = labels[:, 1:]  # decoder output B x seq_len\n",
    "\n",
    "        batch_size, seq_len_src = input_ids.shape\n",
    "        _, seg_len_tgt = tgt_input.shape\n",
    "\n",
    "        # generate positional embedding\n",
    "        src_positions = torch.arange(seq_len_src, device=input_ids.device).unsqueeze(\n",
    "            0\n",
    "        )  # 1 x seq\n",
    "        tgt_positions = torch.arange(seg_len_tgt, device=labels.device).unsqueeze(\n",
    "            0\n",
    "        )  # 1 x seq\n",
    "        \n",
    "        # sum embedding\n",
    "        # (B x seq) + (1 x seq) = embedded\n",
    "        src_embedded = self.embedding_src(input_ids) + self.position_embedding_src(\n",
    "            src_positions\n",
    "        )  # B x seq\n",
    "        tgt_embedded = self.embedding_tgt(tgt_input) + self.position_embedding_tgt(\n",
    "            tgt_positions\n",
    "        )  # B x seq\n",
    "\n",
    "        # generate mask\n",
    "        src_mask, tgt_mask, src_key_padding_mask, tgt_key_padding_mask = self.create_mask(\n",
    "            input_ids, tgt_input\n",
    "        )\n",
    "\n",
    "        # output\n",
    "        output = self.transformer(\n",
    "            src=src_embedded,\n",
    "            tgt=tgt_embedded,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )  # shape (B, Seq, E_dim)\n",
    "  \n",
    "        logits = self.generator(output)  # (B, Seq, Vocab_size)\n",
    "        loss = self.loss_fn(logits.permute(0, 2, 1), tgt_output)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n",
    "\n",
    "    # Define functions for inference phase\n",
    "    def encode(self, src, src_mask, src_padding_mask):\n",
    "        \"\"\"\n",
    "        Inference Encoder, this require a padding mask, if not, significant drop in performance.\n",
    "        \"\"\"\n",
    "        _, src_len_src = src.shape\n",
    "        src_positions = torch.arange(src_len_src, device=src.device).unsqueeze(0)\n",
    "        src_embedded = self.embedding_src(src) + self.position_embedding_src(\n",
    "            src_positions\n",
    "        )\n",
    "        return self.transformer.encoder(src_embedded, src_mask, src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt, encoder_output, causal_mask):\n",
    "        \"\"\"\n",
    "        Inference Decoder, this require a causal mask for auto-regressive,\\\n",
    "        if not, significant drop in performance.\\n\n",
    "        Does not need a padding mask because the model want to predict an \"eos\" token\n",
    "        \"\"\"\n",
    "        _, seq_len_tgt = tgt.shape\n",
    "        tgt_positions = torch.arange(seq_len_tgt, device=tgt.device).unsqueeze(0)\n",
    "        tgt_embedded = self.embedding_tgt(tgt) + self.position_embedding_tgt(\n",
    "            tgt_positions\n",
    "        )\n",
    "        return self.transformer.decoder(tgt_embedded, encoder_output, causal_mask)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\")) # id nào bằng 0 thì chặn không cho tính attention\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        device = src.device\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len, device).to(torch.bool)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "        src_padding_mask = (src == 0) # id nào bằng 0 thì chặn không cho tính attention\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "config = Seq2SeqTransformerConfig(\n",
    "    vocab_size_src=len(tokenizer_en),\n",
    "    vocab_size_tgt=len(tokenizer_vi),\n",
    ")\n",
    "model_transformer = Seq2SeqTransformerModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([preprocessed_ds[\"train\"][10][\"input_ids\"]])\n",
    "labels = torch.tensor([preprocessed_ds[\"train\"][10][\"labels\"]])\n",
    "pred = model_transformer(input_ids=input_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 100000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['logits'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Disable wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./transformer-en-vi\",\n",
    "    logging_dir=\"logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=100,\n",
    "    per_device_eval_batch_size=100,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-05,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    weight_decay=0.01,\n",
    "    #report_to=\"wandb\",\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c201968799444a129861c72ec6b3f184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.3254, 'grad_norm': 1.4536958932876587, 'learning_rate': 2.4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab73d3786f242408b33ddcb46f31275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.299125671386719, 'eval_runtime': 8.0164, 'eval_samples_per_second': 1411.613, 'eval_steps_per_second': 14.221, 'epoch': 1.0}\n",
      "{'loss': 4.1963, 'grad_norm': 1.3414915800094604, 'learning_rate': 1.8e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303ce8d64b344788b37d655f7697bce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.757636785507202, 'eval_runtime': 10.7435, 'eval_samples_per_second': 1053.286, 'eval_steps_per_second': 10.611, 'epoch': 2.0}\n",
      "{'loss': 3.8078, 'grad_norm': 1.3936073780059814, 'learning_rate': 1.2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06e9c67956b4b5c928f2d19bdd46c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4314119815826416, 'eval_runtime': 7.5371, 'eval_samples_per_second': 1501.379, 'eval_steps_per_second': 15.125, 'epoch': 3.0}\n",
      "{'loss': 3.5744, 'grad_norm': 1.6244676113128662, 'learning_rate': 5.999167937872695e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9e2ad0178a495db577d59a08d01a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2314250469207764, 'eval_runtime': 7.7061, 'eval_samples_per_second': 1468.455, 'eval_steps_per_second': 14.794, 'epoch': 4.0}\n",
      "{'loss': 3.4553, 'grad_norm': 1.2886652946472168, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6768753041485aa470b5bdb6a427a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1628687381744385, 'eval_runtime': 7.5341, 'eval_samples_per_second': 1501.969, 'eval_steps_per_second': 15.131, 'epoch': 5.0}\n",
      "{'train_runtime': 27206.1952, 'train_samples_per_second': 530.109, 'train_steps_per_second': 1.325, 'train_loss': 4.07182592133546, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36055, training_loss=4.07182592133546, metrics={'train_runtime': 27206.1952, 'train_samples_per_second': 530.109, 'train_steps_per_second': 1.325, 'total_flos': 6.03894467216448e+17, 'train_loss': 4.07182592133546, 'epoch': 4.999826659733056})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_transformer,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_ds[\"train\"],\n",
    "    eval_dataset=preprocessed_ds[\"validation\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c326614d0a944275adcb60683bf748e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.1766247749328613,\n",
       " 'eval_runtime': 9.5459,\n",
       " 'eval_samples_per_second': 1175.904,\n",
       " 'eval_steps_per_second': 11.838,\n",
       " 'epoch': 4.999826659733056}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(preprocessed_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_transformer.state_dict(), \"./transformer-en-vi/model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.load_state_dict(torch.load(\"./transformer-en-vi/model_weights.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./transformer-en-vi/hub/tokenizer_vi\\\\tokenizer_config.json',\n",
       " './transformer-en-vi/hub/tokenizer_vi\\\\special_tokens_map.json',\n",
       " './transformer-en-vi/hub/tokenizer_vi\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.save_pretrained(\"./transformer-en-vi/hub\")\n",
    "tokenizer_en.save_pretrained(\"./transformer-en-vi/hub/tokenizer_en\")\n",
    "tokenizer_vi.save_pretrained(\"./transformer-en-vi/hub/tokenizer_vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Tạo repository trên Hugging Face\n",
    "model_name = \"binhphap5/en-vi-machine-translation\"\n",
    "api = HfApi()\n",
    "api.create_repo(model_name, exist_ok=True) \n",
    "\n",
    "model_transformer.push_to_hub(model_name)\n",
    "tokenizer_en.push_to_hub(model_name)  \n",
    "tokenizer_vi.push_to_hub(model_name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch translation:\n",
      "1: Con mèo sẽ đến mặt trăng với con tàu.\n",
      "2: Mọi người đều hạnh phúc vì mặt trời mọc.\n",
      "3: Tôi không còn là một đứa trẻ, nhưng tôi vẫn yêu chơi với đồ chơi, lạ phải không?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def batch_beam_search_decode(model, src_sentences, tokenizer_en, tokenizer_vi, beam_width=5, max_len=50, temperature=1, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Thực hiện beam search decode theo batch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    bos_id = tokenizer_vi.bos_token_id\n",
    "    eos_id = tokenizer_vi.eos_token_id\n",
    "\n",
    "    # 1. Tokenize toàn bộ batch với padding cố định\n",
    "    encoded = tokenizer_en.batch_encode_plus(\n",
    "        src_sentences,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    src_tensor = encoded['input_ids'].to(device)  # shape: (B, max_len)\n",
    "    \n",
    "    # Tạo src_mask và src_padding_mask \n",
    "    B, src_seq_len = src_tensor.shape\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=device)\n",
    "    src_padding_mask = (src_tensor == 0)  # True cho các vị trí padding\n",
    "\n",
    "    # 2. Tính encoder output cho toàn batch\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encode(src_tensor, src_mask, src_padding_mask)  \n",
    "        # encoder_output shape: (B, src_seq_len, hidden_dim)\n",
    "\n",
    "    # 3. Khởi tạo beams cho từng câu\n",
    "    # beams: (B, beam_width, current_seq_len), ban đầu current_seq_len=1, chỉ chứa <bos>\n",
    "    beams = torch.full((B, 1, 1), bos_id, dtype=torch.long, device=device)\n",
    "    # scores: (B, beam_width), ban đầu là 0\n",
    "    beam_scores = torch.zeros(B, 1, device=device)\n",
    "    \n",
    "    # Tạo danh sách đánh dấu beam nào đã kết thúc (mỗi câu)\n",
    "    complete_beams = [ [] for _ in range(B) ]\n",
    "    \n",
    "    # Duyệt từng bước sinh token cho đến max_len\n",
    "    for step in range(max_len - 1):  # đã có 1 token ban đầu\n",
    "        B_current, beam_num, seq_len = beams.shape  # B_current == B\n",
    "        # Flatten beams: (B * beam_width, seq_len)\n",
    "        flat_beams = beams.view(B * beam_num, seq_len)\n",
    "        \n",
    "        # Tạo causal mask cho decoder: (seq_len, seq_len)\n",
    "        # Giả sử mô hình có hàm generate_square_subsequent_mask\n",
    "        causal_mask = model.generate_square_subsequent_mask(seq_len, device)  # shape: (seq_len, seq_len)\n",
    "        \n",
    "        # Lặp lại encoder output cho mỗi beam:\n",
    "        # encoder_output: (B, src_seq_len, hidden_dim) -> (B, 1, src_seq_len, hidden_dim) -> (B, beam_width, src_seq_len, hidden_dim)\n",
    "        # Sau đó flatten thành (B * beam_width, src_seq_len, hidden_dim)\n",
    "        repeated_encoder_output = encoder_output.unsqueeze(1).repeat(1, beam_num, 1, 1).view(B * beam_num, src_seq_len, -1)\n",
    "        \n",
    "        #  Tính decoder output cho từng beam (batch mode)\n",
    "        with torch.no_grad():\n",
    "            decoder_output = model.decode(flat_beams, repeated_encoder_output, causal_mask)\n",
    "            # decoder_output shape: (B * beam_width, seq_len, hidden_dim)\n",
    "            logits = model.generator(decoder_output[:, -1, :])  # lấy logit của token mới nhất: (B * beam_width, vocab_size)\n",
    "        \n",
    "        # Áp dụng temperature và tính log softmax\n",
    "        logits = logits / temperature\n",
    "        log_probs = F.log_softmax(logits, dim=-1)  # (B * beam_width, vocab_size)\n",
    "        \n",
    "        # Reshape để tính điểm cho từng beam riêng biệt\n",
    "        log_probs = log_probs.view(B, beam_num, -1)  # (B, beam_width, vocab_size)\n",
    "        \n",
    "        # Cộng dồn điểm của các beam hiện tại với log_probs của token tiếp theo\n",
    "        # beam_scores: (B, beam_width) -> unsqueeze(-1): (B, beam_width, 1)\n",
    "        total_scores = beam_scores.unsqueeze(-1) + log_probs  # (B, beam_width, vocab_size)\n",
    "        \n",
    "        # Reshape lại để chọn top beam_width cho mỗi câu: (B, beam_width * vocab_size)\n",
    "        total_scores = total_scores.view(B, -1)\n",
    "        # Lấy top beam_width chỉ số và điểm\n",
    "        topk_scores, topk_indices = total_scores.topk(beam_width, dim=-1)  # mỗi câu: (beam_width,)\n",
    "        \n",
    "        # Xác định beam index cũ và token mới được chọn\n",
    "        beam_indices = topk_indices // log_probs.size(-1)  # (B, beam_width)\n",
    "        token_indices = topk_indices % log_probs.size(-1)    # (B, beam_width)\n",
    "        \n",
    "        # Cập nhật các beams mới theo beam_indices và token_indices\n",
    "        new_beams = []\n",
    "        new_beam_scores = []\n",
    "        for i in range(B):\n",
    "            beams_i = beams[i]  # (beam_num, seq_len)\n",
    "            new_beams_i = []\n",
    "            new_scores_i = []\n",
    "            for j in range(beam_width):\n",
    "                prev_beam = beams_i[beam_indices[i, j]]\n",
    "                new_token = token_indices[i, j].unsqueeze(0)\n",
    "                new_seq = torch.cat([prev_beam, new_token])\n",
    "                new_beams_i.append(new_seq.unsqueeze(0))\n",
    "                new_scores_i.append(topk_scores[i, j].unsqueeze(0))\n",
    "            # Coi new_beams_i là tensor (beam_width, seq_len_new)\n",
    "            new_beams.append(torch.cat(new_beams_i, dim=0).unsqueeze(0))\n",
    "            new_beam_scores.append(torch.cat(new_scores_i, dim=0).unsqueeze(0))\n",
    "        # Ghép lại theo batch\n",
    "        beams = torch.cat(new_beams, dim=0)  # (B, beam_width, seq_len_new)\n",
    "        beam_scores = torch.cat(new_beam_scores, dim=0)  # (B, beam_width)\n",
    "        \n",
    "        # Kiểm tra các beam đã kết thúc (nếu token cuối là eos)\n",
    "        # Lưu lại các beam đã hoàn thành và đặt điểm của chúng thành -inf để không chọn tiếp\n",
    "        beams_list = []\n",
    "        scores_list = []\n",
    "        for i in range(B):\n",
    "            beams_i = beams[i]\n",
    "            scores_i = beam_scores[i]\n",
    "            ongoing_beams = []\n",
    "            ongoing_scores = []\n",
    "            for j in range(beam_width):\n",
    "                if beams_i[j, -1].item() == eos_id:\n",
    "                    complete_beams[i].append((beams_i[j], scores_i[j]))\n",
    "                else:\n",
    "                    ongoing_beams.append(beams_i[j].unsqueeze(0))\n",
    "                    ongoing_scores.append(scores_i[j].unsqueeze(0))\n",
    "            # Nếu không còn beam nào chưa kết thúc, giữ lại beam tốt nhất (để vòng lặp không dừng)\n",
    "            if len(ongoing_beams) == 0:\n",
    "                ongoing_beams = [beams_i[0].unsqueeze(0)]\n",
    "                ongoing_scores = [scores_i[0].unsqueeze(0)]\n",
    "            beams_list.append(torch.cat(ongoing_beams, dim=0))\n",
    "            scores_list.append(torch.cat(ongoing_scores, dim=0))\n",
    "        # Cập nhật beams và beam_scores sau khi lọc theo từng câu\n",
    "        # Lưu ý: số beam có thể khác nhau giữa các câu, ta cần pad lại về beam_width\n",
    "        new_beams = []\n",
    "        new_scores = []\n",
    "        for i in range(B):\n",
    "            cur_beams = beams_list[i]\n",
    "            cur_scores = scores_list[i]\n",
    "            cur_beam_num = cur_beams.shape[0]\n",
    "            if cur_beam_num < beam_width:\n",
    "                # Nếu số beam ít hơn beam_width, ta có thể pad thêm các giá trị rất thấp để tránh bị chọn\n",
    "                pad_num = beam_width - cur_beam_num\n",
    "                pad_seq = cur_beams[0].unsqueeze(0).repeat(pad_num, 1)\n",
    "                pad_scores = torch.full((pad_num,), -1e9, device=device)\n",
    "                cur_beams = torch.cat([cur_beams, pad_seq], dim=0)\n",
    "                cur_scores = torch.cat([cur_scores, pad_scores], dim=0)\n",
    "            new_beams.append(cur_beams.unsqueeze(0))\n",
    "            new_scores.append(cur_scores.unsqueeze(0))\n",
    "        beams = torch.cat(new_beams, dim=0)  # (B, beam_width, seq_len_new)\n",
    "        beam_scores = torch.cat(new_scores, dim=0)  # (B, beam_width)\n",
    "        \n",
    "        # Nếu với mỗi câu, tất cả các beam đều đã kết thúc, ta có thể dừng sớm\n",
    "        if all(len(complete_beams[i]) >= beam_width for i in range(B)):\n",
    "            break\n",
    "\n",
    "    # Chọn kết quả tốt nhất cho mỗi câu\n",
    "    final_translations = []\n",
    "    for i in range(B):\n",
    "        if complete_beams[i]:\n",
    "            # Lấy beam có điểm cao nhất\n",
    "            best_beam = max(complete_beams[i], key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            best_beam = beams[i][0]\n",
    "        # Giải mã token sang câu\n",
    "        translation = tokenizer_vi.decode(best_beam.tolist(), skip_special_tokens=True)\n",
    "        final_translations.append(translation)\n",
    "        \n",
    "    return final_translations\n",
    "\n",
    "# Example usage:\n",
    "src_sentences = [\"A cat is going to the moon with its ship.\",\n",
    "                \"Everyone is happy because the sun is shining.\",\n",
    "                \"I am no longer a child, but I still love to play with toys, strange right?\",]\n",
    "translations = batch_beam_search_decode(model_transformer, src_sentences, tokenizer_en, tokenizer_vi, beam_width=5, max_len=50, temperature=1, device=\"cuda\")\n",
    "# free up VRAM\n",
    "torch.cuda.empty_cache()\n",
    "print(\"batch translation:\")\n",
    "for idx, trans in enumerate(translations):\n",
    "    print(f\"{idx+1}: {trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128 / 11225\n",
      "Processed 256 / 11225\n",
      "Processed 384 / 11225\n",
      "Processed 512 / 11225\n",
      "Processed 640 / 11225\n",
      "Processed 768 / 11225\n",
      "Processed 896 / 11225\n",
      "Processed 1024 / 11225\n",
      "Processed 1152 / 11225\n",
      "Processed 1280 / 11225\n",
      "Processed 1408 / 11225\n",
      "Processed 1536 / 11225\n",
      "Processed 1664 / 11225\n",
      "Processed 1792 / 11225\n",
      "Processed 1920 / 11225\n",
      "Processed 2048 / 11225\n",
      "Processed 2176 / 11225\n",
      "Processed 2304 / 11225\n",
      "Processed 2432 / 11225\n",
      "Processed 2560 / 11225\n",
      "Processed 2688 / 11225\n",
      "Processed 2816 / 11225\n",
      "Processed 2944 / 11225\n",
      "Processed 3072 / 11225\n",
      "Processed 3200 / 11225\n",
      "Processed 3328 / 11225\n",
      "Processed 3456 / 11225\n",
      "Processed 3584 / 11225\n",
      "Processed 3712 / 11225\n",
      "Processed 3840 / 11225\n",
      "Processed 3968 / 11225\n",
      "Processed 4096 / 11225\n",
      "Processed 4224 / 11225\n",
      "Processed 4352 / 11225\n",
      "Processed 4480 / 11225\n",
      "Processed 4608 / 11225\n",
      "Processed 4736 / 11225\n",
      "Processed 4864 / 11225\n",
      "Processed 4992 / 11225\n",
      "Processed 5120 / 11225\n",
      "Processed 5248 / 11225\n",
      "Processed 5376 / 11225\n",
      "Processed 5504 / 11225\n",
      "Processed 5632 / 11225\n",
      "Processed 5760 / 11225\n",
      "Processed 5888 / 11225\n",
      "Processed 6016 / 11225\n",
      "Processed 6144 / 11225\n",
      "Processed 6272 / 11225\n",
      "Processed 6400 / 11225\n",
      "Processed 6528 / 11225\n",
      "Processed 6656 / 11225\n",
      "Processed 6784 / 11225\n",
      "Processed 6912 / 11225\n",
      "Processed 7040 / 11225\n",
      "Processed 7168 / 11225\n",
      "Processed 7296 / 11225\n",
      "Processed 7424 / 11225\n",
      "Processed 7552 / 11225\n",
      "Processed 7680 / 11225\n",
      "Processed 7808 / 11225\n",
      "Processed 7936 / 11225\n",
      "Processed 8064 / 11225\n",
      "Processed 8192 / 11225\n",
      "Processed 8320 / 11225\n",
      "Processed 8448 / 11225\n",
      "Processed 8576 / 11225\n",
      "Processed 8704 / 11225\n",
      "Processed 8832 / 11225\n",
      "Processed 8960 / 11225\n",
      "Processed 9088 / 11225\n",
      "Processed 9216 / 11225\n",
      "Processed 9344 / 11225\n",
      "Processed 9472 / 11225\n",
      "Processed 9600 / 11225\n",
      "Processed 9728 / 11225\n",
      "Processed 9856 / 11225\n",
      "Processed 9984 / 11225\n",
      "Processed 10112 / 11225\n",
      "Processed 10240 / 11225\n",
      "Processed 10368 / 11225\n",
      "Processed 10496 / 11225\n",
      "Processed 10624 / 11225\n",
      "Processed 10752 / 11225\n",
      "Processed 10880 / 11225\n",
      "Processed 11008 / 11225\n",
      "Processed 11136 / 11225\n",
      "Processed 11225 / 11225\n",
      "\n",
      "BLEU score trên tập test: 0.2503\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def compute_bleu_score(model, test_dataset, tokenizer_en, tokenizer_vi, beam_width=5, max_len=50, temperature=1, device=\"cuda\", batch_size=128):\n",
    "    \"\"\"\n",
    "    Tính BLEU score cho tập test bằng cách chia thành các batch nhỏ.\n",
    "    \n",
    "    test_dataset: dict có ít nhất 2 trường 'en' và 'vi'\n",
    "    \"\"\"\n",
    "    src_sentences = test_dataset['en']  # danh sách câu tiếng Anh\n",
    "    target_sentences = test_dataset['vi']  # danh sách câu tiếng Việt tham chiếu\n",
    "\n",
    "    all_predictions = []\n",
    "    n_samples = len(src_sentences)\n",
    "    # Chia tập test thành các batch nhỏ theo batch_size\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        batch_src = src_sentences[i:i+batch_size]\n",
    "        # Gọi hàm dịch theo batch đã định nghĩa\n",
    "        batch_predictions = batch_beam_search_decode(model, batch_src, tokenizer_en, tokenizer_vi,\n",
    "                                                       beam_width=beam_width, max_len=max_len,\n",
    "                                                       temperature=temperature, device=device)\n",
    "        all_predictions.extend(batch_predictions)\n",
    "        print(f\"Processed {min(i+batch_size, n_samples)} / {n_samples}\")\n",
    "\n",
    "    # Chuẩn hóa câu: tách token theo khoảng trắng\n",
    "    # Yêu cầu của corpus_bleu: danh sách tham chiếu cho mỗi câu dưới dạng list[list[str]]\n",
    "    references = [[ref.split()] for ref in target_sentences]\n",
    "    hypotheses = [pred.split() for pred in all_predictions]\n",
    "\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "\n",
    "# Giả sử preprocessed_ds['test'] chứa các trường 'en', 'vi'\n",
    "bleu = compute_bleu_score(model_transformer, preprocessed_ds['test'], tokenizer_en, tokenizer_vi,\n",
    "                          beam_width=5, max_len=50, temperature=1, device=\"cuda\", batch_size=128)\n",
    "# free up VRAM\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nBLEU score trên tập test: {bleu:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
