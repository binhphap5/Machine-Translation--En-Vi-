{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers, trainers, models\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ncduy/mt-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 2884451\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 11316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 11225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 2884451\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 11316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 11225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.remove_columns([\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MAX_LEN = 50 \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = examples[\"en\"]\n",
    "    tgt_texts = examples[\"vi\"]\n",
    "    src_encodings = tokenizer(src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    tgt_encodings = tokenizer(tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    return {\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "preprocessed_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast\n",
    "\n",
    "MAX_LEN = 50  \n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"vi_VN\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = examples[\"en\"]\n",
    "    tgt_texts = examples[\"vi\"]\n",
    "    src_encodings = tokenizer(src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tgt_encodings = tokenizer(tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    return {\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "preprocessed_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_XX'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(250004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ds['train'][20]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "model_mbart = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([preprocessed_ds[\"train\"][10][\"input_ids\"]])\n",
    "labels = torch.tensor([preprocessed_ds[\"train\"][10][\"labels\"]])\n",
    "pred = model_mbart(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 250054])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['logits'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart-50-en-vi\",\n",
    "    logging_dir=\"logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-05,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.05,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    #report_to=\"wandb\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1577061cc4e464f84a470d44debe62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6184, 'grad_norm': 1.205190658569336, 'learning_rate': 6.219338429813704e-12, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696501e12a1d4ebd911265569323a995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4920928478240967, 'eval_runtime': 27.8232, 'eval_samples_per_second': 406.711, 'eval_steps_per_second': 25.446, 'epoch': 1.0}\n",
      "{'train_runtime': 39861.936, 'train_samples_per_second': 72.361, 'train_steps_per_second': 2.261, 'train_loss': 0.6184480098098493, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90139, training_loss=0.6184480098098493, metrics={'train_runtime': 39861.936, 'train_samples_per_second': 72.361, 'train_steps_per_second': 2.261, 'total_flos': 3.052232971124736e+17, 'train_loss': 0.6184480098098493, 'epoch': 0.999994453042229})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_mbart,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_ds[\"train\"],\n",
    "    eval_dataset=preprocessed_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=tokenizer(\"I am going to bed\", return_tensors=\"pt\")['input_ids'].to(model_mbart.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model_mbart.generate(input, max_length=50, num_beams=5, early_stopping=True, temperature=1.0, do_sample=True, forced_bos_token_id=tokenizer.lang_code_to_id[\"vi_VN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁đang'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(4724)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 250024,    384,  38068,   4724,   2467,  27421,   4600,      5,\n",
       "              2]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tớ đang đi ngủ đây.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f819234fcf0f40b594c2325fcce950cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb781af7d641d5ad4884de73cd67cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3379b5fbaee441689fac05f8138eb096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5220383ed33548ff9ed5bf1f1e6077af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b798c5d818942cc8c9b2d54551b0dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/binhphap5/mbart-50-en-vi/commit/41bc2dee30026ba8283440d56276a8f174ce3b0c', commit_message='mBART-50 EN-VI', commit_description='', oid='41bc2dee30026ba8283440d56276a8f174ce3b0c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/binhphap5/mbart-50-en-vi', endpoint='https://huggingface.co', repo_type='model', repo_id='binhphap5/mbart-50-en-vi'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"mBART-50 EN-VI\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cad918e3f2c49238879d5cf50b848c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.49249181151390076,\n",
       " 'eval_runtime': 28.4953,\n",
       " 'eval_samples_per_second': 393.924,\n",
       " 'eval_steps_per_second': 24.636,\n",
       " 'epoch': 0.999994453042229}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(preprocessed_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Một con mèo sẽ lên mặt trăng cùng các phi hành gia'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "translator = pipeline(\n",
    "    model=\"binhphap5/mbart-50-en-vi\",\n",
    "    device=device,\n",
    ")\n",
    "translator(\n",
    "    \"A cat is going to the moon with the astronauts\",\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    temperature=1,\n",
    "    do_sample=True,\n",
    ")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32 / 11225\n",
      "Processed 64 / 11225\n",
      "Processed 96 / 11225\n",
      "Processed 128 / 11225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 160 / 11225\n",
      "Processed 192 / 11225\n",
      "Processed 224 / 11225\n",
      "Processed 256 / 11225\n",
      "Processed 288 / 11225\n",
      "Processed 320 / 11225\n",
      "Processed 352 / 11225\n",
      "Processed 384 / 11225\n",
      "Processed 416 / 11225\n",
      "Processed 448 / 11225\n",
      "Processed 480 / 11225\n",
      "Processed 512 / 11225\n",
      "Processed 544 / 11225\n",
      "Processed 576 / 11225\n",
      "Processed 608 / 11225\n",
      "Processed 640 / 11225\n",
      "Processed 672 / 11225\n",
      "Processed 704 / 11225\n",
      "Processed 736 / 11225\n",
      "Processed 768 / 11225\n",
      "Processed 800 / 11225\n",
      "Processed 832 / 11225\n",
      "Processed 864 / 11225\n",
      "Processed 896 / 11225\n",
      "Processed 928 / 11225\n",
      "Processed 960 / 11225\n",
      "Processed 992 / 11225\n",
      "Processed 1024 / 11225\n",
      "Processed 1056 / 11225\n",
      "Processed 1088 / 11225\n",
      "Processed 1120 / 11225\n",
      "Processed 1152 / 11225\n",
      "Processed 1184 / 11225\n",
      "Processed 1216 / 11225\n",
      "Processed 1248 / 11225\n",
      "Processed 1280 / 11225\n",
      "Processed 1312 / 11225\n",
      "Processed 1344 / 11225\n",
      "Processed 1376 / 11225\n",
      "Processed 1408 / 11225\n",
      "Processed 1440 / 11225\n",
      "Processed 1472 / 11225\n",
      "Processed 1504 / 11225\n",
      "Processed 1536 / 11225\n",
      "Processed 1568 / 11225\n",
      "Processed 1600 / 11225\n",
      "Processed 1632 / 11225\n",
      "Processed 1664 / 11225\n",
      "Processed 1696 / 11225\n",
      "Processed 1728 / 11225\n",
      "Processed 1760 / 11225\n",
      "Processed 1792 / 11225\n",
      "Processed 1824 / 11225\n",
      "Processed 1856 / 11225\n",
      "Processed 1888 / 11225\n",
      "Processed 1920 / 11225\n",
      "Processed 1952 / 11225\n",
      "Processed 1984 / 11225\n",
      "Processed 2016 / 11225\n",
      "Processed 2048 / 11225\n",
      "Processed 2080 / 11225\n",
      "Processed 2112 / 11225\n",
      "Processed 2144 / 11225\n",
      "Processed 2176 / 11225\n",
      "Processed 2208 / 11225\n",
      "Processed 2240 / 11225\n",
      "Processed 2272 / 11225\n",
      "Processed 2304 / 11225\n",
      "Processed 2336 / 11225\n",
      "Processed 2368 / 11225\n",
      "Processed 2400 / 11225\n",
      "Processed 2432 / 11225\n",
      "Processed 2464 / 11225\n",
      "Processed 2496 / 11225\n",
      "Processed 2528 / 11225\n",
      "Processed 2560 / 11225\n",
      "Processed 2592 / 11225\n",
      "Processed 2624 / 11225\n",
      "Processed 2656 / 11225\n",
      "Processed 2688 / 11225\n",
      "Processed 2720 / 11225\n",
      "Processed 2752 / 11225\n",
      "Processed 2784 / 11225\n",
      "Processed 2816 / 11225\n",
      "Processed 2848 / 11225\n",
      "Processed 2880 / 11225\n",
      "Processed 2912 / 11225\n",
      "Processed 2944 / 11225\n",
      "Processed 2976 / 11225\n",
      "Processed 3008 / 11225\n",
      "Processed 3040 / 11225\n",
      "Processed 3072 / 11225\n",
      "Processed 3104 / 11225\n",
      "Processed 3136 / 11225\n",
      "Processed 3168 / 11225\n",
      "Processed 3200 / 11225\n",
      "Processed 3232 / 11225\n",
      "Processed 3264 / 11225\n",
      "Processed 3296 / 11225\n",
      "Processed 3328 / 11225\n",
      "Processed 3360 / 11225\n",
      "Processed 3392 / 11225\n",
      "Processed 3424 / 11225\n",
      "Processed 3456 / 11225\n",
      "Processed 3488 / 11225\n",
      "Processed 3520 / 11225\n",
      "Processed 3552 / 11225\n",
      "Processed 3584 / 11225\n",
      "Processed 3616 / 11225\n",
      "Processed 3648 / 11225\n",
      "Processed 3680 / 11225\n",
      "Processed 3712 / 11225\n",
      "Processed 3744 / 11225\n",
      "Processed 3776 / 11225\n",
      "Processed 3808 / 11225\n",
      "Processed 3840 / 11225\n",
      "Processed 3872 / 11225\n",
      "Processed 3904 / 11225\n",
      "Processed 3936 / 11225\n",
      "Processed 3968 / 11225\n",
      "Processed 4000 / 11225\n",
      "Processed 4032 / 11225\n",
      "Processed 4064 / 11225\n",
      "Processed 4096 / 11225\n",
      "Processed 4128 / 11225\n",
      "Processed 4160 / 11225\n",
      "Processed 4192 / 11225\n",
      "Processed 4224 / 11225\n",
      "Processed 4256 / 11225\n",
      "Processed 4288 / 11225\n",
      "Processed 4320 / 11225\n",
      "Processed 4352 / 11225\n",
      "Processed 4384 / 11225\n",
      "Processed 4416 / 11225\n",
      "Processed 4448 / 11225\n",
      "Processed 4480 / 11225\n",
      "Processed 4512 / 11225\n",
      "Processed 4544 / 11225\n",
      "Processed 4576 / 11225\n",
      "Processed 4608 / 11225\n",
      "Processed 4640 / 11225\n",
      "Processed 4672 / 11225\n",
      "Processed 4704 / 11225\n",
      "Processed 4736 / 11225\n",
      "Processed 4768 / 11225\n",
      "Processed 4800 / 11225\n",
      "Processed 4832 / 11225\n",
      "Processed 4864 / 11225\n",
      "Processed 4896 / 11225\n",
      "Processed 4928 / 11225\n",
      "Processed 4960 / 11225\n",
      "Processed 4992 / 11225\n",
      "Processed 5024 / 11225\n",
      "Processed 5056 / 11225\n",
      "Processed 5088 / 11225\n",
      "Processed 5120 / 11225\n",
      "Processed 5152 / 11225\n",
      "Processed 5184 / 11225\n",
      "Processed 5216 / 11225\n",
      "Processed 5248 / 11225\n",
      "Processed 5280 / 11225\n",
      "Processed 5312 / 11225\n",
      "Processed 5344 / 11225\n",
      "Processed 5376 / 11225\n",
      "Processed 5408 / 11225\n",
      "Processed 5440 / 11225\n",
      "Processed 5472 / 11225\n",
      "Processed 5504 / 11225\n",
      "Processed 5536 / 11225\n",
      "Processed 5568 / 11225\n",
      "Processed 5600 / 11225\n",
      "Processed 5632 / 11225\n",
      "Processed 5664 / 11225\n",
      "Processed 5696 / 11225\n",
      "Processed 5728 / 11225\n",
      "Processed 5760 / 11225\n",
      "Processed 5792 / 11225\n",
      "Processed 5824 / 11225\n",
      "Processed 5856 / 11225\n",
      "Processed 5888 / 11225\n",
      "Processed 5920 / 11225\n",
      "Processed 5952 / 11225\n",
      "Processed 5984 / 11225\n",
      "Processed 6016 / 11225\n",
      "Processed 6048 / 11225\n",
      "Processed 6080 / 11225\n",
      "Processed 6112 / 11225\n",
      "Processed 6144 / 11225\n",
      "Processed 6176 / 11225\n",
      "Processed 6208 / 11225\n",
      "Processed 6240 / 11225\n",
      "Processed 6272 / 11225\n",
      "Processed 6304 / 11225\n",
      "Processed 6336 / 11225\n",
      "Processed 6368 / 11225\n",
      "Processed 6400 / 11225\n",
      "Processed 6432 / 11225\n",
      "Processed 6464 / 11225\n",
      "Processed 6496 / 11225\n",
      "Processed 6528 / 11225\n",
      "Processed 6560 / 11225\n",
      "Processed 6592 / 11225\n",
      "Processed 6624 / 11225\n",
      "Processed 6656 / 11225\n",
      "Processed 6688 / 11225\n",
      "Processed 6720 / 11225\n",
      "Processed 6752 / 11225\n",
      "Processed 6784 / 11225\n",
      "Processed 6816 / 11225\n",
      "Processed 6848 / 11225\n",
      "Processed 6880 / 11225\n",
      "Processed 6912 / 11225\n",
      "Processed 6944 / 11225\n",
      "Processed 6976 / 11225\n",
      "Processed 7008 / 11225\n",
      "Processed 7040 / 11225\n",
      "Processed 7072 / 11225\n",
      "Processed 7104 / 11225\n",
      "Processed 7136 / 11225\n",
      "Processed 7168 / 11225\n",
      "Processed 7200 / 11225\n",
      "Processed 7232 / 11225\n",
      "Processed 7264 / 11225\n",
      "Processed 7296 / 11225\n",
      "Processed 7328 / 11225\n",
      "Processed 7360 / 11225\n",
      "Processed 7392 / 11225\n",
      "Processed 7424 / 11225\n",
      "Processed 7456 / 11225\n",
      "Processed 7488 / 11225\n",
      "Processed 7520 / 11225\n",
      "Processed 7552 / 11225\n",
      "Processed 7584 / 11225\n",
      "Processed 7616 / 11225\n",
      "Processed 7648 / 11225\n",
      "Processed 7680 / 11225\n",
      "Processed 7712 / 11225\n",
      "Processed 7744 / 11225\n",
      "Processed 7776 / 11225\n",
      "Processed 7808 / 11225\n",
      "Processed 7840 / 11225\n",
      "Processed 7872 / 11225\n",
      "Processed 7904 / 11225\n",
      "Processed 7936 / 11225\n",
      "Processed 7968 / 11225\n",
      "Processed 8000 / 11225\n",
      "Processed 8032 / 11225\n",
      "Processed 8064 / 11225\n",
      "Processed 8096 / 11225\n",
      "Processed 8128 / 11225\n",
      "Processed 8160 / 11225\n",
      "Processed 8192 / 11225\n",
      "Processed 8224 / 11225\n",
      "Processed 8256 / 11225\n",
      "Processed 8288 / 11225\n",
      "Processed 8320 / 11225\n",
      "Processed 8352 / 11225\n",
      "Processed 8384 / 11225\n",
      "Processed 8416 / 11225\n",
      "Processed 8448 / 11225\n",
      "Processed 8480 / 11225\n",
      "Processed 8512 / 11225\n",
      "Processed 8544 / 11225\n",
      "Processed 8576 / 11225\n",
      "Processed 8608 / 11225\n",
      "Processed 8640 / 11225\n",
      "Processed 8672 / 11225\n",
      "Processed 8704 / 11225\n",
      "Processed 8736 / 11225\n",
      "Processed 8768 / 11225\n",
      "Processed 8800 / 11225\n",
      "Processed 8832 / 11225\n",
      "Processed 8864 / 11225\n",
      "Processed 8896 / 11225\n",
      "Processed 8928 / 11225\n",
      "Processed 8960 / 11225\n",
      "Processed 8992 / 11225\n",
      "Processed 9024 / 11225\n",
      "Processed 9056 / 11225\n",
      "Processed 9088 / 11225\n",
      "Processed 9120 / 11225\n",
      "Processed 9152 / 11225\n",
      "Processed 9184 / 11225\n",
      "Processed 9216 / 11225\n",
      "Processed 9248 / 11225\n",
      "Processed 9280 / 11225\n",
      "Processed 9312 / 11225\n",
      "Processed 9344 / 11225\n",
      "Processed 9376 / 11225\n",
      "Processed 9408 / 11225\n",
      "Processed 9440 / 11225\n",
      "Processed 9472 / 11225\n",
      "Processed 9504 / 11225\n",
      "Processed 9536 / 11225\n",
      "Processed 9568 / 11225\n",
      "Processed 9600 / 11225\n",
      "Processed 9632 / 11225\n",
      "Processed 9664 / 11225\n",
      "Processed 9696 / 11225\n",
      "Processed 9728 / 11225\n",
      "Processed 9760 / 11225\n",
      "Processed 9792 / 11225\n",
      "Processed 9824 / 11225\n",
      "Processed 9856 / 11225\n",
      "Processed 9888 / 11225\n",
      "Processed 9920 / 11225\n",
      "Processed 9952 / 11225\n",
      "Processed 9984 / 11225\n",
      "Processed 10016 / 11225\n",
      "Processed 10048 / 11225\n",
      "Processed 10080 / 11225\n",
      "Processed 10112 / 11225\n",
      "Processed 10144 / 11225\n",
      "Processed 10176 / 11225\n",
      "Processed 10208 / 11225\n",
      "Processed 10240 / 11225\n",
      "Processed 10272 / 11225\n",
      "Processed 10304 / 11225\n",
      "Processed 10336 / 11225\n",
      "Processed 10368 / 11225\n",
      "Processed 10400 / 11225\n",
      "Processed 10432 / 11225\n",
      "Processed 10464 / 11225\n",
      "Processed 10496 / 11225\n",
      "Processed 10528 / 11225\n",
      "Processed 10560 / 11225\n",
      "Processed 10592 / 11225\n",
      "Processed 10624 / 11225\n",
      "Processed 10656 / 11225\n",
      "Processed 10688 / 11225\n",
      "Processed 10720 / 11225\n",
      "Processed 10752 / 11225\n",
      "Processed 10784 / 11225\n",
      "Processed 10816 / 11225\n",
      "Processed 10848 / 11225\n",
      "Processed 10880 / 11225\n",
      "Processed 10912 / 11225\n",
      "Processed 10944 / 11225\n",
      "Processed 10976 / 11225\n",
      "Processed 11008 / 11225\n",
      "Processed 11040 / 11225\n",
      "Processed 11072 / 11225\n",
      "Processed 11104 / 11225\n",
      "Processed 11136 / 11225\n",
      "Processed 11168 / 11225\n",
      "Processed 11200 / 11225\n",
      "Processed 11225 / 11225\n",
      "\n",
      "BLEU score (sử dụng sacrebleu): 42.38\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def compute_bleu_score_pipeline_sacrebleu(translator, test_dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Tính BLEU score sử dụng sacrebleu cho pipeline HuggingFace.\n",
    "    \n",
    "    Args:\n",
    "        translator: pipeline dịch máy HuggingFace\n",
    "        test_dataset: dict với khóa 'en' và 'vi'\n",
    "        batch_size: số câu xử lý mỗi lần\n",
    "    \n",
    "    Returns:\n",
    "        BLEU score float\n",
    "    \"\"\"\n",
    "    src_sentences = test_dataset['en']\n",
    "    tgt_sentences = test_dataset['vi']\n",
    "\n",
    "    predictions = []\n",
    "    n = len(src_sentences)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = src_sentences[i:i+batch_size]\n",
    "        results = translator(\n",
    "            batch,\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            temperature=1,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        preds = [res[\"generated_text\"] for res in results]\n",
    "        predictions.extend(preds)\n",
    "        print(f\"Processed {min(i+batch_size, n)} / {n}\")\n",
    "\n",
    "    # Tính BLEU bằng sacrebleu\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [tgt_sentences])\n",
    "    return bleu.score\n",
    "\n",
    "bleu_score = compute_bleu_score_pipeline_sacrebleu(translator, preprocessed_ds['test'], batch_size=32)\n",
    "# free up VRAM\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nBLEU score (sử dụng sacrebleu): {bleu_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
