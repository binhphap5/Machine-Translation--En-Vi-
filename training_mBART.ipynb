{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers, trainers, models\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ncduy/mt-en-vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 2884451\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 11316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi', 'source'],\n",
       "        num_rows: 11225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 2884451\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 11316\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 11225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.remove_columns([\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word - based\n",
    "tokenizer_en = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer_vi = Tokenizer(models.WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer_vi.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
    ")\n",
    "# train tokenizer   \n",
    "tokenizer_en.train_from_iterator(ds[\"train\"][\"en\"], trainer)\n",
    "tokenizer_vi.train_from_iterator(ds[\"train\"][\"vi\"], trainer)\n",
    "# tokenizer\n",
    "tokenizer_en.save(\"tokenizer_en.json\")\n",
    "tokenizer_vi.save(\"tokenizer_vi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_en = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_en.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "tokenizer_vi = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer_vi.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = examples[\"en\"]\n",
    "    tgt_texts = [\"<bos>\" + sent + \"<eos>\" for sent in examples[\"vi\"]]\n",
    "    src_encodings = tokenizer_en(\n",
    "        src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN\n",
    "    )\n",
    "    tgt_encodings = tokenizer_vi(\n",
    "        tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "preprocessed_ds = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "def is_valid_sample(sample):\n",
    "    return any(token != 0 for token in sample[\"input_ids\"])\n",
    "\n",
    "preprocessed_ds = preprocessed_ds.filter(is_valid_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Smallpox also ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlán alone, including the emperor, and Peru in the 1530s, aiding the European conquerors.',\n",
       " 'vi': 'Bệnh đậu mùa cũng tàn phá México vào những năm 1520, chỉ riêng người Tenochtitlán đã có hơn 150.000 người chết, gồm cả quốc vương, và Peru vào những năm 1530, nhờ đó hỗ trợ cho những người châu Âu đi chinh phục.',\n",
       " 'source': 'WikiMatrix v1',\n",
       " 'input_ids': [1,\n",
       "  85,\n",
       "  18437,\n",
       "  1270,\n",
       "  12,\n",
       "  6,\n",
       "  1,\n",
       "  5,\n",
       "  1520,\n",
       "  2447,\n",
       "  5,\n",
       "  196,\n",
       "  12,\n",
       "  1,\n",
       "  727,\n",
       "  5,\n",
       "  253,\n",
       "  6,\n",
       "  2390,\n",
       "  5,\n",
       "  10,\n",
       "  3480,\n",
       "  12,\n",
       "  6,\n",
       "  1,\n",
       "  5,\n",
       "  17500,\n",
       "  6,\n",
       "  675,\n",
       "  34857,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [2,\n",
       "  1918,\n",
       "  1955,\n",
       "  530,\n",
       "  61,\n",
       "  1336,\n",
       "  517,\n",
       "  2668,\n",
       "  29,\n",
       "  28,\n",
       "  20,\n",
       "  13604,\n",
       "  5,\n",
       "  76,\n",
       "  605,\n",
       "  17,\n",
       "  1,\n",
       "  13,\n",
       "  10,\n",
       "  74,\n",
       "  2341,\n",
       "  4,\n",
       "  492,\n",
       "  17,\n",
       "  273,\n",
       "  5,\n",
       "  255,\n",
       "  73,\n",
       "  158,\n",
       "  905,\n",
       "  5,\n",
       "  8,\n",
       "  2392,\n",
       "  29,\n",
       "  28,\n",
       "  20,\n",
       "  11487,\n",
       "  5,\n",
       "  1115,\n",
       "  22,\n",
       "  851,\n",
       "  525,\n",
       "  18,\n",
       "  28,\n",
       "  17,\n",
       "  520,\n",
       "  634,\n",
       "  59,\n",
       "  1696,\n",
       "  439]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_ds['train'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MAX_LEN = 50 \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = examples[\"en\"]\n",
    "    tgt_texts = examples[\"vi\"]\n",
    "    src_encodings = tokenizer(src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    tgt_encodings = tokenizer(tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    return {\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "preprocessed_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0c710e5da14f18bd8dba3d291fc65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast\n",
    "\n",
    "MAX_LEN = 50  \n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"vi_VN\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = examples[\"en\"]\n",
    "    tgt_texts = examples[\"vi\"]\n",
    "    src_encodings = tokenizer(src_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tgt_encodings = tokenizer(tgt_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    return {\n",
    "        \"input_ids\": src_encodings[\"input_ids\"],\n",
    "        \"labels\": tgt_encodings[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "preprocessed_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_XX'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(250004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ds['train'][20]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "model_mbart = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([preprocessed_ds[\"train\"][10][\"input_ids\"]])\n",
    "labels = torch.tensor([preprocessed_ds[\"train\"][10][\"labels\"]])\n",
    "pred = model_mbart(input_ids=input_ids, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Disable wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# Training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart-50-en-vi\",\n",
    "    logging_dir=\"logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-05,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.05,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    #report_to=\"wandb\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1577061cc4e464f84a470d44debe62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6184, 'grad_norm': 1.205190658569336, 'learning_rate': 6.219338429813704e-12, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696501e12a1d4ebd911265569323a995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4920928478240967, 'eval_runtime': 27.8232, 'eval_samples_per_second': 406.711, 'eval_steps_per_second': 25.446, 'epoch': 1.0}\n",
      "{'train_runtime': 39861.936, 'train_samples_per_second': 72.361, 'train_steps_per_second': 2.261, 'train_loss': 0.6184480098098493, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90139, training_loss=0.6184480098098493, metrics={'train_runtime': 39861.936, 'train_samples_per_second': 72.361, 'train_steps_per_second': 2.261, 'total_flos': 3.052232971124736e+17, 'train_loss': 0.6184480098098493, 'epoch': 0.999994453042229})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_mbart,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_ds[\"train\"],\n",
    "    eval_dataset=preprocessed_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=tokenizer(\"I am going to bed\", return_tensors=\"pt\")['input_ids'].to(model_mbart.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model_mbart.generate(input, max_length=50, num_beams=5, early_stopping=True, temperature=1.0, do_sample=True, forced_bos_token_id=tokenizer.lang_code_to_id[\"vi_VN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁đang'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(4724)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 250024,    384,  38068,   4724,   2467,  27421,   4600,      5,\n",
       "              2]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tớ đang đi ngủ đây.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f819234fcf0f40b594c2325fcce950cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb781af7d641d5ad4884de73cd67cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3379b5fbaee441689fac05f8138eb096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5220383ed33548ff9ed5bf1f1e6077af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b798c5d818942cc8c9b2d54551b0dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/binhphap5/mbart-50-en-vi/commit/41bc2dee30026ba8283440d56276a8f174ce3b0c', commit_message='mBART-50 EN-VI', commit_description='', oid='41bc2dee30026ba8283440d56276a8f174ce3b0c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/binhphap5/mbart-50-en-vi', endpoint='https://huggingface.co', repo_type='model', repo_id='binhphap5/mbart-50-en-vi'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"mBART-50 EN-VI\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cad918e3f2c49238879d5cf50b848c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.49249181151390076,\n",
       " 'eval_runtime': 28.4953,\n",
       " 'eval_samples_per_second': 393.924,\n",
       " 'eval_steps_per_second': 24.636,\n",
       " 'epoch': 0.999994453042229}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(preprocessed_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Một con mèo sẽ lên mặt trăng với các phi hành gia'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "translator = pipeline(\n",
    "    model=\"binhphap5/mbart-50-en-vi\",\n",
    "    device=device,\n",
    ")\n",
    "translator(\n",
    "    \"A cat is going to the moon with the astronauts\",\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    temperature=1,\n",
    "    do_sample=True,\n",
    ")[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
